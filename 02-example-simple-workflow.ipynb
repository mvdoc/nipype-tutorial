{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our first nipype workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning objectives of this notebook: understanding how to link nodes together to write a simple workflow.**\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think of a nipype workflow (or any neuroimaging pipeline) as a [Directed Acyclic Graph (or DAG)](https://en.wikipedia.org/wiki/Directed_acyclic_graph), or a series of operations that can be performed serially or in parallel, and in which loops are not allowed (that is, the output of a child cannot be the input of a parent). Think of water flowing down a mountain slope (your input data), splitting into smaller rivers (your processes), and ending into a lake (the output data).\n",
    "\n",
    "In Nipype, each vertex in the graph is either a `Node` object, which wraps an `Interface`, or another `Workflow`. Note that by using other workflows as vertices, nipype can generate modular pipelines, which will be useful when developing more complicated pipelines. In addition, it makes testing the individual parts of your pipeline much easier.\n",
    "\n",
    "Writing nipype workflows is fairly straightforward after you have enough practice with the syntax. In my experience, the best way to write a workflow is by first drawing it on paper, and then moving to the code. Let's write a very simple pipeline that does the following on the input data\n",
    "\n",
    "1. perform motion correction\n",
    "2. estimate TSNR on the motion-corrected data\n",
    "3. take the motion correction parameters and estimate framewise displacement\n",
    "\n",
    "Or as a graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from nxpd import draw\n",
    "G = nx.DiGraph()\n",
    "G.graph['dpi'] = 120\n",
    "G.add_nodes_from(['Input', 'Motion Correction', 'TSNR', 'Framewise Displacement', 'Output'])\n",
    "G.add_edges_from([\n",
    "    ('Input', 'Motion Correction'),\n",
    "    ('Motion Correction', 'TSNR'),\n",
    "    ('Motion Correction', 'Framewise Displacement'),\n",
    "    ('TSNR', 'Output'),\n",
    "    ('Motion Correction', 'Output'),\n",
    "    ('Framewise Displacement', 'Output')\n",
    "])\n",
    "draw(G, show='ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nipype import Workflow, Node, IdentityInterface\n",
    "from nipype.interfaces import afni\n",
    "from nipype.interfaces import fsl\n",
    "from nipype.algorithms.confounds import FramewiseDisplacement, TSNR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to setup an input node and an output node. For this special type of node we are going to use an `IdentityInterface`. This interface simply creates identity mappings. They are used to collect input and output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputnode = Node(\n",
    "    IdentityInterface(\n",
    "        # fields specify what input/output fields this node will have.\n",
    "        # In this case, the node `inputnode` will have an input called `in_file` \n",
    "        # and an output called `in_file`\n",
    "        fields=['in_file']),\n",
    "    name='inputnode'\n",
    ")\n",
    "\n",
    "outputnode = Node(\n",
    "    IdentityInterface(fields=['tsnr_file', 'moco_file', 'fd']),\n",
    "    name='outputnode'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can create the intermediate nodes that will perform actions on the input data. We will use FSL's `MCFLIRT` for motion correction, AFNI's `3dTstat` for computing tSNR, and Nipype's `algorithms.confounds.FramewiseDisplacement` to compute framewise displacement from MCFLIRT's motion parameters. This example also shows you how easy it is to mix and match tools from different neuroimaging toolkits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flirt = Node(\n",
    "    fsl.MCFLIRT(output_type='NIFTI_GZ', save_plots=True), \n",
    "    name='flirt'\n",
    ")\n",
    "fd = Node(\n",
    "    FramewiseDisplacement(parameter_source='FSL'),\n",
    "    name='compute_fd'\n",
    ")\n",
    "\n",
    "# we are going to use afni to compute tSNR\n",
    "# the option -cvarinv of 3dTstat computes tSNR after linear detrending\n",
    "tsnr = Node(\n",
    "    afni.TStat(outputtype='NIFTI_GZ', args='-cvarinv'),\n",
    "    name='compute_tsnr'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to link all these nodes together. First, we create an instance of a `Workflow` object. The only mandatory argument is its name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf = Workflow(name='my_first_workflow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we are ready to connect the nodes using the method `Workflow.connect`. This method can be used in two ways. In its simplest form, the syntax is\n",
    "\n",
    "```python\n",
    "Workflow.connect(in_node, \"output_field\", out_node, \"input_field\")\n",
    "```\n",
    "\n",
    "that is, we are linking `in_node` to `out_node` by passing `\"output_field\"` (of `in_node`) to `\"input_field\"` (of `out_node`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first way to connect nodes\n",
    "# wf.connect(in_node, output_field, out_node, input_field)\n",
    "#\n",
    "# input data -> flirt\n",
    "wf.connect(inputnode, 'in_file', flirt, 'in_file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, if we need to link many fields of the same node to another node, we can use the syntax\n",
    "\n",
    "```python\n",
    "Workflow.connect([  # this is a list of tuple\n",
    "    # each tuple has three elements\n",
    "    # the first two elements are the input and output nodes\n",
    "    (in_node, out_node, # <-- which nodes to connect?\n",
    "       # the last element is a list of tuples indicating the fields to connect\n",
    "       [(\"output_field_1\", \"input_field_1\"),  # <-- which fields to connect?\n",
    "        (\"output_field_2\", \"input_field_2\")])\n",
    "])\n",
    "```\n",
    "\n",
    "This is very confusing at first, and you might get lost in brackets (use an IDE for this). However, when you get used to it, it becomes very efficient and you will never use the first syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf.connect([\n",
    "    # moco params -> framewise displacement\n",
    "    (flirt, fd, [('par_file', 'in_file')]),\n",
    "    # moco file -> tsnr\n",
    "    (flirt, tsnr, [('out_file', 'in_file')]),\n",
    "    # moco file -> output node\n",
    "    (flirt, outputnode, [('out_file', 'moco_file')]),\n",
    "    # tsnr file -> output node\n",
    "    (tsnr, outputnode, [('out_file', 'tsnr_file')]),\n",
    "    # framewise displacement -> output node\n",
    "    (fd, outputnode, [('out_file', 'fd')])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. We created our first workflow! We can visualize it to make sure we have connected everything properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf.write_graph('myfirstworkflow')\n",
    "\n",
    "from IPython.display import Image\n",
    "Image('myfirstworkflow.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks OK! Let's now get some data and see if it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download some data from OpenNeuro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to download some data from OpenNeuro using [DataLad](https://www.datalad.org/). This is not necessary for Nipype, but I hope these three lines of code will pique your curiosity about the wonderful possibilities of DataLad ;-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need the following to fix asyncio issues with jupyter\n",
    "# see https://github.com/datalad/datalad/issues/5409\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# import datalad to download the data\n",
    "import datalad.api as dl\n",
    "\n",
    "# Download one of my datasets from OpenNeuro (\"The Grand Budapest Hotel\")\n",
    "# https://openneuro.org/datasets/ds003017\n",
    "# We will be using datalad for this\n",
    "dl.install('budapest-data', '///openneuro/ds003017')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what we have downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls budapest-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree budapest-data/sub-sid000021 | head -20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now we have only downloaded a list of files, without the actual file content (the content is saved to git with [git-annex](https://git-annex.branchable.com)). To get the data, we need to tell datalad to download the file we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just get one subject's run.\n",
    "data_fn = 'budapest-data/sub-sid000021/func/sub-sid000021_task-movie_run-01_bold.nii.gz'\n",
    "dl.get(data_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if we have a brain in there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import cortex\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = nib.load(data_fn).get_fdata()\n",
    "print(f\"Shape of data is {data.shape}\")\n",
    "# compute mean across TRs\n",
    "data_avg = data.mean(-1)\n",
    "# plot mosaic\n",
    "plt.figure(figsize=(12, 12))\n",
    "_ = cortex.mosaic(data_avg[..., 10:40].T, cmap='gray', vmin=0, vmax=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GOOD, it looks like a brain! We are going to select the first 50 TRs to make our example run faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.image import index_img\n",
    "import os\n",
    "\n",
    "# do not use relative paths\n",
    "data_fn = os.path.abspath(data_fn)\n",
    "# get the first 50 TRs\n",
    "data_subset = index_img(data_fn, slice(0, 50))\n",
    "\n",
    "print(f\"Shape of data_subset is {data_subset.shape}\")\n",
    "# save to test_data\n",
    "test_data_fn = os.path.abspath('test_data.nii.gz')\n",
    "data_subset.to_filename(test_data_fn)\n",
    "\n",
    "print(f\"Test data is in {test_data_fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's run our workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default our workflow will run in the current directory. This is not great. We can set a work directory where all the intermediate files will be stored. This directory will be used for caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a \"work directory\"\n",
    "wf.base_dir = os.path.abspath('./workdir')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to pass our inputfile to the workflow. We do so by setting the `in_file` field of the `inputnode`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we are selecting the inputs first (wf.inputs),\n",
    "# then we are specifying which node we want to modify (wf.inputs.inputnode),\n",
    "# and finally we set the parameter in_file \n",
    "wf.inputs.inputnode.in_file = test_data_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we are ready to run our workflow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run it -- by default it's going to run sequentially with one thread\n",
    "wf.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all our results are under the working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree workdir/my_first_workflow/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each node (except the `inputnode` and the `outputnode`) has an associated folder containing the node outputs. For now we are going to ignore the other files. However, note that nodes wrapping command line arguments (such as `flirt`) contain a file named `command.txt`. This file contains the command that was run to obtain the output file. For debugging, it's often useful to check this command and run it in a terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat workdir/my_first_workflow/flirt/command.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the output tSNR volume and the framewise displacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsnr_img = nib.load('workdir/my_first_workflow/compute_tsnr/test_data_mcf_tstat.nii.gz')\n",
    "tsnr_data = tsnr_img.get_fdata()\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "_ = cortex.mosaic(tsnr_data.T, cmap='plasma', vmin=0, vmax=130)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "fd_data = np.loadtxt('workdir/my_first_workflow/compute_fd/fd_power_2012.txt', skiprows=1)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 2))\n",
    "ax.plot(fd_data)\n",
    "ax.set_ylim([0, .4])\n",
    "ax.set_ylabel('FD [mm]')\n",
    "ax.set_xlabel('TR')\n",
    "ax.grid(axis='y')\n",
    "_ = ax.set_title('Framewise Displacement')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "In this notebook we created our first workflow. However, we are not done yet. We often want to apply the same workflow to multiple input files. And we do not need to dig files out of the work directory every time we run our pipelines. In the next notebook, we will extend our simple pipeline to run it over multiple input files and we will generate a nicely-formatted output directory for all our output files.\n",
    "\n",
    "Go to the next notebook: [03-extending-our-first-workflow](03-extending-our-first-workflow.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
