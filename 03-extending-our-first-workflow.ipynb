{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending our first workflow\n",
    "\n",
    "**Learning objectives of this notebook:**\n",
    "\n",
    "- creating nodes that run arbitrary python functions\n",
    "- applying the same workflow to multiple input files\n",
    "- generating an output folder with a `DataSink`\n",
    "\n",
    "***\n",
    "\n",
    "In the previous notebook we created our first simple workflow that runs the following steps:\n",
    "\n",
    "1. perform motion correction\n",
    "2. estimate TSNR on the motion-corrected data\n",
    "3. take the motion correction parameters and estimate framewise displacement\n",
    "\n",
    "This workflow could be used as an initial quality-assurance check on all the data you collect. To do so, we need to make the workflow a bit more sophisticated. In particular, we will\n",
    "\n",
    "1. create a node that will extract the first 50 volumes from the input data (note that you don't want to do this if you were to run the workflow as a QA step; we are doing it here to keep the running time of the workflow manageable)\n",
    "2. modify the code so that the workflow can be applied to any number of input files\n",
    "3. collect all output data in a single output directory\n",
    "\n",
    "\n",
    "## Setup\n",
    "\n",
    "In the next cells we are just going to import what we need, and create the nodes that we will use in our workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from nipype import Workflow, Node, IdentityInterface, Function\n",
    "\n",
    "from nipype.interfaces import afni\n",
    "from nipype.interfaces import fsl\n",
    "from nipype.algorithms.confounds import FramewiseDisplacement\n",
    "\n",
    "inputnode = Node(\n",
    "    IdentityInterface(\n",
    "        fields=['in_file']),\n",
    "    name='inputnode'\n",
    ")\n",
    "\n",
    "outputnode = Node(\n",
    "    IdentityInterface(fields=['tsnr_file', 'moco_file', 'fd']),\n",
    "    name='outputnode'\n",
    ")\n",
    "\n",
    "flirt = Node(\n",
    "    fsl.MCFLIRT(output_type='NIFTI_GZ', save_plots=True), \n",
    "    name='flirt'\n",
    ")\n",
    "fd = Node(\n",
    "    FramewiseDisplacement(parameter_source='FSL'),\n",
    "    name='compute_fd'\n",
    ")\n",
    "\n",
    "tsnr = Node(\n",
    "    afni.TStat(outputtype='NIFTI_GZ', args='-cvarinv'),\n",
    "    name='compute_tsnr'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the first 50 volumes of the input data with a node\n",
    "\n",
    "Recall that to keep the workflow running in a short time, we manually extracted the first 50 volumes of the input data. This is something you might want to do when you are debugging your workflow. \n",
    "\n",
    "We are going to create a node that performs the same operation with nilearn. We will use the special interface `Function`, which is able to run any arbitrary python code as a new interface. First, let's create a python function that given an input file, selects the first 50 volumes, saves the reduced file, and returns the filename of the new file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_50_volumes(in_file):\n",
    "    # NOTE: all imports **must** be inside the function that you create\n",
    "    # this is because the process running the function does not inherit\n",
    "    # the imports from the parent process\n",
    "    from nilearn.image import index_img\n",
    "    import os\n",
    "    \n",
    "    # it's always good practice to deal with absolute paths\n",
    "    in_file = os.path.abspath(in_file)\n",
    "    # get the first 50 TRs\n",
    "    data_subset = index_img(in_file, slice(0, 50))\n",
    "    # generate a new output filename\n",
    "    # if the input file is called `infile.nii.gz`\n",
    "    # the output file will be `infile_50vol.nii.gz`\n",
    "    out_file = os.path.basename(in_file)\n",
    "    out_file = out_file.replace('.nii.gz', '_50vol.nii.gz')\n",
    "    # NOTE: the output file will be relative to the working directory\n",
    "    # of the node. You should always return an absolute path\n",
    "    out_file = os.path.abspath(out_file)\n",
    "    # save\n",
    "    data_subset.to_filename(out_file)\n",
    "    # the function should (almost always) return the filename of the output file\n",
    "    return out_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we created the function, we can wrap it in a `Function` interface "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_volumes_interface = Function(\n",
    "    # NOTE: the input names MUST match the name of your args and kwargs exactly\n",
    "    input_names=['in_file'],\n",
    "    output_names=['out_file'],\n",
    "    function=select_50_volumes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we make it a `Node`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_volumes = Node(select_volumes_interface, name='select_volumes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to plug this node into our workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf = Workflow(name='my_second_workflow')\n",
    "\n",
    "wf.connect([\n",
    "    # input file -> select volumes\n",
    "    (inputnode, select_volumes, [('in_file', 'in_file')]),\n",
    "    # select volumes -> motion correction\n",
    "    (select_volumes, flirt, [('out_file', 'in_file')]),\n",
    "    # moco params -> framewise displacement\n",
    "    (flirt, fd, [('par_file', 'in_file')]),\n",
    "    # moco file -> tsnr\n",
    "    (flirt, tsnr, [('out_file', 'in_file')]),\n",
    "    # moco file -> output node\n",
    "    (flirt, outputnode, [('out_file', 'moco_file')]),\n",
    "    # tsnr file -> output node\n",
    "    (tsnr, outputnode, [('out_file', 'tsnr_file')]),\n",
    "    # framewise displacement -> output node\n",
    "    (fd, outputnode, [('out_file', 'fd')])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare our two workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf.write_graph('mysecondworkflow')\n",
    "\n",
    "from IPython.display import Image\n",
    "Image('mysecondworkflow.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('myfirstworkflow.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our second workflow has an extra node between the `inputnode` and the `flirt` node called `select_volumes`, which will run the function `select_50_volumes`. Let's run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use our previously downloaded data\n",
    "data_fn = 'budapest-data/sub-sid000021/func/sub-sid000021_task-movie_run-01_bold.nii.gz'\n",
    "\n",
    "# set up a \"work directory\"\n",
    "wf.base_dir = os.path.abspath('./workdir')\n",
    "\n",
    "# pass the input file\n",
    "wf.inputs.inputnode.in_file = os.path.abspath(data_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run it\n",
    "wf.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, it ran without errors! Let's see what the outputs are in our working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree workdir/my_second_workflow/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there's an extra folder called `select_volumes` with an output called `sub-sid000021_task-movie_run-01_bold_50vol.nii.gz`. Let's check the shape of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "\n",
    "fn = 'workdir/my_second_workflow/select_volumes/sub-sid000021_task-movie_run-01_bold_50vol.nii.gz'\n",
    "img = nib.load(fn)\n",
    "print(f\"{fn.split('/')[-1]} has shape {img.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our node with custom python code worked!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating our workflow over multiple files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a small (but useful!) workflow to generate some QA files. However, our workflow takes a single file as an input. There's no reason why we shouldn't be able to apply this workflow to any number of input files. There are multiple ways to accomplish this. I will show my favorite way (which is inspired by how it's coded in [fMRIPrep](https://fmriprep.org/en/stable/)). It's by no means the \"correct\" way.\n",
    "\n",
    "\n",
    "### Fixing output filenames\n",
    "First, we need to fix the output filenames of our current workflow. When we want to iterate the same workflow over a number of input files, it's better if the output files are unique. All our nodes except one are generating unique output filenames (scroll up to see what the work directory contains). We need to fix the node that computes framewise displacement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree workdir/my_second_workflow/compute_fd/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example the input file is called `sub-sid000021_task-movie_run-01_bold.nii.gz`, but the output file of this node is called `fd_power_2012.txt`. A more reasonable (and unique) output filename would be `sub-sid000021_task-movie_run-01_fd.txt`. How to do this?\n",
    "\n",
    "Let's check the parameters of the interface `FramewiseDisplacement`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FramewiseDisplacement.help()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that it has an optional input parameter called `out_file`. This parameter by default is set to `fd_power_2012.txt`, but it can be changed to any string we want. \n",
    "\n",
    "For simplicity we will assume that our input file is formatted according to the [BIDS standard](https://bids-specification.readthedocs.io/en/stable/). Thus, given an input file such as `sub-sid000021_task-movie_run-01_bold.nii.gz`, we can generate a unique output file for the framewise displacement node by replacing `_bold.nii.gz` with `_fd.txt`. \n",
    "\n",
    "This can be easily accomplished using an advanced feature of Nipype, which allows arbitrary functions to be applied to any output of a node. The syntax can be a bit (more) convoluted, but it will become natural with enough practice.\n",
    "\n",
    "Say that we are linking our `inputnode` to the `compute_fd` node by passing `inputnode.in_file` to `compute_fd.out_file`. Without any extra operation, we would connect them as follows:\n",
    "\n",
    "```python\n",
    "wf.connect([\n",
    "    (inputnode, fd, [('in_file', 'out_file')])\n",
    "])\n",
    "```\n",
    "\n",
    "Now say we have defined a function that performs the string replacement that we want:\n",
    "\n",
    "```python\n",
    "def make_fd_outfile(fn):\n",
    "    # NOTE: imports must be defined inside the function\n",
    "    import os\n",
    "    # take only the basename, otherwise the output will be\n",
    "    # in the same directory as the input filename\n",
    "    fn = os.path.basename(fn)\n",
    "    fn = fn.replace('_bold.nii.gz', '_fd.txt')\n",
    "    return fn\n",
    "```\n",
    "\n",
    "we can apply this function to `inputnode.in_file` with the following syntax:\n",
    "\n",
    "```python\n",
    "wf.connect([\n",
    "    (inputnode, fd, [(('in_file', make_fd_outfile), 'out_file')])\n",
    "])\n",
    "```\n",
    "\n",
    "note that we have replaced `'in_file'` with the tuple `('in_file', make_fd_outfile)`. This will tell Nipype to take `inputnode.in_file`, pass it to `make_fd_outfile`, and then pass the output to `compute_fd.out_file`. \n",
    "\n",
    "Let's try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fd_outfile(fn):\n",
    "    # NOTE: imports must be defined inside the function\n",
    "    import os\n",
    "    # take only the basename, otherwise the output will be\n",
    "    # in the same directory as the input filename\n",
    "    fn = os.path.basename(fn)\n",
    "    fn = fn.replace('_bold.nii.gz', '_fd.txt')\n",
    "    return fn\n",
    "\n",
    "wf = Workflow(name='my_second_workflow_v2')\n",
    "\n",
    "# make nodes\n",
    "inputnode = Node(\n",
    "    IdentityInterface(\n",
    "        fields=['in_file']),\n",
    "    name='inputnode'\n",
    ")\n",
    "\n",
    "outputnode = Node(\n",
    "    IdentityInterface(fields=['tsnr_file', 'moco_file', 'fd']),\n",
    "    name='outputnode'\n",
    ")\n",
    "\n",
    "flirt = Node(\n",
    "    fsl.MCFLIRT(output_type='NIFTI_GZ', save_plots=True), \n",
    "    name='flirt'\n",
    ")\n",
    "fd = Node(\n",
    "    FramewiseDisplacement(parameter_source='FSL'),\n",
    "    name='compute_fd'\n",
    ")\n",
    "\n",
    "tsnr = Node(\n",
    "    afni.TStat(outputtype='NIFTI_GZ', args='-cvarinv'),\n",
    "    name='compute_tsnr'\n",
    ")\n",
    "\n",
    "select_volumes_interface = Function(\n",
    "    # NOTE: the input names MUST match the name of your args and kwargs exactly\n",
    "    input_names=['in_file'],\n",
    "    output_names=['out_file'],\n",
    "    function=select_50_volumes\n",
    ")\n",
    "select_volumes = Node(select_volumes_interface, name='select_volumes')\n",
    "\n",
    "wf.connect([\n",
    "    # input file -> select volumes\n",
    "    (inputnode, select_volumes, [('in_file', 'in_file')]),\n",
    "    # input_file -> fd out_file to generate unique filename\n",
    "    (inputnode, fd, [(('in_file', make_fd_outfile), 'out_file')]),\n",
    "    # select volumes -> motion correction\n",
    "    (select_volumes, flirt, [('out_file', 'in_file')]),\n",
    "    # moco params -> framewise displacement\n",
    "    (flirt, fd, [('par_file', 'in_file')]),\n",
    "    # moco file -> tsnr\n",
    "    (flirt, tsnr, [('out_file', 'in_file')]),\n",
    "    # moco file -> output node\n",
    "    (flirt, outputnode, [('out_file', 'moco_file')]),\n",
    "    # tsnr file -> output node\n",
    "    (tsnr, outputnode, [('out_file', 'tsnr_file')]),\n",
    "    # framewise displacement -> output node\n",
    "    (fd, outputnode, [('out_file', 'fd')])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how this new workflow looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf.write_graph('mysecondworkflow_v2')\n",
    "\n",
    "from IPython.display import Image\n",
    "Image('mysecondworkflow_v2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's try to run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set up a \"work directory\"\n",
    "wf.base_dir = os.path.abspath('./workdir')\n",
    "\n",
    "# pass the input file\n",
    "wf.inputs.inputnode.in_file = os.path.abspath(data_fn)\n",
    "wf.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the outputs now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree workdir/my_second_workflow_v2/compute_fd/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now this workflow will generate unique output filenames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looping over multiple input files\n",
    "\n",
    "To loop over multiple input files, we will first create a \"parent\" (or main) workflow. Then, for every input file, we will create a subworkflow that will process the input file. We will add the subworkflow to the parent workflow. In this way, we will be able to use the parallel processing feature of Nipype. \n",
    "\n",
    "Let's get started.\n",
    "\n",
    "First, we'll create a function that initializes a new subworkflow. This subworkflow will perform the processes that we tested so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fd_outfile(fn):\n",
    "    # NOTE: imports must be defined inside the function\n",
    "    import os\n",
    "    # take only the basename, otherwise the output will be\n",
    "    # in the same directory as the input filename\n",
    "    fn = os.path.basename(fn)\n",
    "    fn = fn.replace('_bold.nii.gz', '_fd.txt')\n",
    "    return fn\n",
    "\n",
    "\n",
    "# Note that we are simply wrapping our previous code in a function\n",
    "# and adding a docstring :-)\n",
    "def init_qa_wf(name):\n",
    "    \"\"\"Initialize a QA workflow that computes temporal SNR on the first 50 motion-corrected \n",
    "    volumes of the input file. It also computes framewise displacement.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    name : str\n",
    "        name of the workflow\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    wf : nipype.Workflow\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    in_file\n",
    "        Path to the input EPI file.\n",
    " \n",
    "    Outputs\n",
    "    -------\n",
    "    tsnr_file\n",
    "        tSNR volume of the first 50 TRs.\n",
    "    moco_file\n",
    "        Motion-corrected volume.\n",
    "    fd\n",
    "        File containing framewise displacement values.\n",
    "    \"\"\"\n",
    "    wf = Workflow(name=name)\n",
    "    # make nodes\n",
    "    inputnode = Node(\n",
    "        IdentityInterface(\n",
    "            fields=['in_file']),\n",
    "        name='inputnode'\n",
    "    )\n",
    "\n",
    "    outputnode = Node(\n",
    "        IdentityInterface(fields=['tsnr_file', 'moco_file', 'fd']),\n",
    "        name='outputnode'\n",
    "    )\n",
    "\n",
    "    flirt = Node(\n",
    "        fsl.MCFLIRT(output_type='NIFTI_GZ', save_plots=True), \n",
    "        name='flirt'\n",
    "    )\n",
    "    fd = Node(\n",
    "        FramewiseDisplacement(parameter_source='FSL'),\n",
    "        name='compute_fd'\n",
    "    )\n",
    "\n",
    "    tsnr = Node(\n",
    "        afni.TStat(outputtype='NIFTI_GZ', args='-cvarinv'),\n",
    "        name='compute_tsnr'\n",
    "    )\n",
    "\n",
    "    select_volumes_interface = Function(\n",
    "        # NOTE: the input names MUST match the name of your args and kwargs exactly\n",
    "        input_names=['in_file'],\n",
    "        output_names=['out_file'],\n",
    "        function=select_50_volumes\n",
    "    )\n",
    "    select_volumes = Node(select_volumes_interface, name='select_volumes')\n",
    "\n",
    "    # connect everything\n",
    "    wf.connect([\n",
    "        # input file -> select volumes\n",
    "        (inputnode, select_volumes, [('in_file', 'in_file')]),\n",
    "        # input_file -> fd out_file to generate unique filename\n",
    "        (inputnode, fd, [(('in_file', make_fd_outfile), 'out_file')]),\n",
    "        # select volumes -> motion correction\n",
    "        (select_volumes, flirt, [('out_file', 'in_file')]),\n",
    "        # moco params -> framewise displacement\n",
    "        (flirt, fd, [('par_file', 'in_file')]),\n",
    "        # moco file -> tsnr\n",
    "        (flirt, tsnr, [('out_file', 'in_file')]),\n",
    "        # moco file -> output node\n",
    "        (flirt, outputnode, [('out_file', 'moco_file')]),\n",
    "        # tsnr file -> output node\n",
    "        (tsnr, outputnode, [('out_file', 'tsnr_file')]),\n",
    "        # framewise displacement -> output node\n",
    "        (fd, outputnode, [('out_file', 'fd')])\n",
    "    ])\n",
    "    return wf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this function, we will loop through all our input files, generate a new subworkflow, and add it to the parent workflow.\n",
    "\n",
    "Let's get first some input files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "in_files = sorted(glob('budapest-data/sub-sid000021/func/sub-sid000021_task-movie_run-*_bold.nii.gz'))\n",
    "print('\\n'.join(in_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a new subworkflow for each of these input files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_wf = Workflow('my_main_workflow')\n",
    "for in_file in in_files:\n",
    "    # Let's generate a short unique name for each input file\n",
    "    wf_name = os.path.basename(in_file).replace('_bold.nii.gz', '_wf')\n",
    "    # remove dashes to clear up the name\n",
    "    wf_name = wf_name.replace('-', '')\n",
    "    print(f\"Creating workflow {wf_name}\")\n",
    "    # Create a subworkflow\n",
    "    wf = init_qa_wf(wf_name)\n",
    "    # Pass the input file\n",
    "    wf.inputs.inputnode.in_file = os.path.abspath(in_file)\n",
    "    # add to the main workflow\n",
    "    main_wf.add_nodes([wf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see what we have created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_wf.write_graph('mymainworkflow_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('mymainworkflow_v1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, that looks reasonable. We have created five copies of the same workflow. Now we are ready to run it in parallel using multiprocessing. But first, we need to get the data with datalad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datalad.api as dl\n",
    "_ = dl.get(in_files)  # do not return anything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we are ready to run the workflow. Note that we are running this workflow using the `MultiProc` plugin, which will use multiprocessing. See [this tutorial](https://miykael.github.io/nipype_tutorial/notebooks/basic_plugins.html) for other available plugins (such as slurm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# first set the work directory\n",
    "main_wf.base_dir = './workdir'\n",
    "main_wf.run(\n",
    "    # which plugin to use,\n",
    "    # see also https://miykael.github.io/nipype_tutorial/notebooks/basic_plugins.html\n",
    "    plugin='MultiProc', \n",
    "    plugin_args={'n_procs': 6}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the output in the work directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!tree workdir/my_main_workflow/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, it looks like all the outputs were generated correctly. We are now one step closer to finishing our pipeline. We just need to collect everything in a single output directory, so we don't have to deal with navigating the work directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a DataSink to collect outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To collect all our outputs in a single output directory, we can use a special interface called `DataSink`. This interface will collect in a directory whatever output of a node we decide to link to it. Thus, we only need to modify our code slightly to collect all the outputs from each subworkflow into the `DataSink`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataSink node\n",
    "from nipype.interfaces import DataSink\n",
    "sinker = Node(DataSink(), 'sinker')\n",
    "\n",
    "# This is going to be our output directory\n",
    "sinker.inputs.base_directory = os.path.abspath('derivatives')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: most of this code is the same as before\n",
    "main_wf = Workflow('my_main_workflow')\n",
    "for in_file in in_files:\n",
    "    # Let's generate a short unique name for each input file\n",
    "    wf_name = os.path.basename(in_file).replace('_bold.nii.gz', '_wf')\n",
    "    # remove dashes to clear up the name\n",
    "    wf_name = wf_name.replace('-', '')\n",
    "    print(f\"Creating workflow {wf_name}\")\n",
    "    # Create a subworkflow\n",
    "    wf = init_qa_wf(wf_name)\n",
    "    # Pass the input file\n",
    "    wf.inputs.inputnode.in_file = os.path.abspath(in_file)\n",
    "    # add to the main workflow\n",
    "    main_wf.add_nodes([wf])\n",
    "    #########################################################################\n",
    "    # NEW CODE!\n",
    "    # \n",
    "    # After adding the subworkflow to the main workflow, we will link\n",
    "    # the outputs to the sinker\n",
    "    main_wf.connect([\n",
    "        (wf, sinker, [('outputnode.fd', f'qa.@{wf_name}_fd'),\n",
    "                      ('outputnode.moco_file', f'qa.@{wf_name}_moco_file'),\n",
    "                      ('outputnode.tsnr_file', f'qa.@{wf_name}_tsnr_file')])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the syntax of the `DataSink`:\n",
    "\n",
    "```python\n",
    "    main_wf.connect([\n",
    "        (wf, sinker, [('outputnode.fd', f'qa.@{wf_name}_fd'),\n",
    "                      ('outputnode.moco_file', f'qa.@{wf_name}_moco_file'),\n",
    "                      ('outputnode.tsnr_file', f'qa.@{wf_name}_tsnr_file')])\n",
    "    ])\n",
    "```\n",
    "\n",
    "For every subworkflow `wf`, we are connecting each of the slots in the outputnode to a **uniquely named** slot in the `DataSink`. For example, for the last subworkflow we are linking `outputnode.fd` to `qa.@subsid000021_taskmovie_run05_wf_fd`. The name of the slot of the `DataSink` specifies which folders to create. Any substring (split by `.`) without an `@` sign will create a new subfolder. In the previous case, `outputnode.fd` will be saved to `./derivatives/qa`. If instead we specified `qa.subsid000021_taskmovie_run05_wf_fd` (without the `@`), `outputnode.fd` would be saved to `./derivatives/qa/subsid000021_taskmovie_run05_wf_fd`.\n",
    "\n",
    "Let's check what the final workflow looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_wf.write_graph('mymainworkflow_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('mymainworkflow_v2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks good. All our subworkflows are connected to the `DataSink` through each `outputnode`. Let's run this workflow again and see what gets saved in `./derivatives`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_wf.base_dir = './workdir/'\n",
    "main_wf.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You probably noticed that this time the workflow ran really fast. This is because all the output nodes were already cached in the work directory, so the only node that had to be run was the `DataSink` node.\n",
    "\n",
    "Let's check the derivatives folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree derivatives/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is looking good, and we could stop here. However, some filenames are a bit confusing. For example, we might want the motion corrected outputs to be called\n",
    "\n",
    "```\n",
    "sub-sid000021_task-movie_run-0[1-5]_bold_moco.nii.gz\n",
    "```\n",
    "\n",
    "We might also want the filename of the tSNR volumes to be more informative, like\n",
    "\n",
    "```\n",
    "sub-sid000021_task-movie_run-0[1-5]_bold_tsnr.nii.gz\n",
    "```\n",
    "\n",
    "We can achieve this by specifying string substitutions in the `DataSink`, so that the output filenames will be better named. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataSink node\n",
    "sinker = Node(DataSink(), 'sinker')\n",
    "# This is going to be our output directory\n",
    "sinker.inputs.base_directory = os.path.abspath('derivatives2')\n",
    "\n",
    "# Specify string substitutions. These are simply tuples indicating\n",
    "# ('substring', 'replacement')\n",
    "# as you would specify when using the `str.replace` method.\n",
    "# \n",
    "# Note that the substitutions will be run in the order in which they are \n",
    "# passed.\n",
    "sinker.inputs.substitutions = [\n",
    "    ('50vol_mcf_tstat', 'tsnr'),\n",
    "    ('50vol_mcf', 'moco')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: most of this code is the same as before\n",
    "main_wf = Workflow('my_main_workflow')\n",
    "for in_file in in_files:\n",
    "    # Let's generate a short unique name for each input file\n",
    "    wf_name = os.path.basename(in_file).replace('_bold.nii.gz', '_wf')\n",
    "    # remove dashes to clear up the name\n",
    "    wf_name = wf_name.replace('-', '')\n",
    "    print(f\"Creating workflow {wf_name}\")\n",
    "    # Create a subworkflow\n",
    "    wf = init_qa_wf(wf_name)\n",
    "    # Pass the input file\n",
    "    wf.inputs.inputnode.in_file = os.path.abspath(in_file)\n",
    "    # add to the main workflow\n",
    "    main_wf.add_nodes([wf])\n",
    "    # After adding the subworkflow to the main workflow, we will link\n",
    "    # the outputs to the sinker\n",
    "    main_wf.connect([\n",
    "        (wf, sinker, [('outputnode.fd', f'qa.@{wf_name}_fd'),\n",
    "                      ('outputnode.moco_file', f'qa.@{wf_name}_moco_file'),\n",
    "                      ('outputnode.tsnr_file', f'qa.@{wf_name}_tsnr_file')])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let's run the workflow\n",
    "main_wf.base_dir = './workdir/'\n",
    "main_wf.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we are ready to check the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree derivatives2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks really good and organized :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A final step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final step, let's just check what we have generated. Let's plot the framewise displacement for each run (for the first 50 volumes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fd_files = sorted(glob('derivatives2/qa/*.txt'))\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 2))\n",
    "\n",
    "for i, fd_fn in enumerate(fd_files, 1):\n",
    "    fd_data = np.loadtxt(fd_fn, skiprows=1)\n",
    "    ax.plot(fd_data, label=f'Run {i}')\n",
    "ax.set_ylim([0, .4])\n",
    "ax.set_ylabel('FD [mm]')\n",
    "ax.set_xlabel('TR')\n",
    "ax.grid(axis='y')\n",
    "ax.legend()\n",
    "_ = ax.set_title('Framewise Displacement')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
